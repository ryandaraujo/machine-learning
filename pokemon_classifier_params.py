# -*- coding: utf-8 -*-
"""pokemon-classifier_params.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VAh3tVuhymzXRjRA5Z_lLoakCTs0Z5te
"""

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from google.colab import drive

#Carrega o Drive
drive.mount("/content/gdrive")

#Pega o dataset de https://www.kaggle.com/datasets/maca11/all-pokemon-dataset
DATASET_PATH = "gdrive/My Drive/Colab Notebooks/all_pokemon_data.csv"
data = pd.read_csv(DATASET_PATH)

#Pré-processamento (campos)
#Inteiros
data["Legendary Status"] = data["Legendary Status"].astype(int)

numerical_columns = [
    "Base Stat Total", "Health", "Attack", "Defense", "Speed", "Special Attack", "Special Defense"
]
for col in numerical_columns:
    data[col] = pd.to_numeric(data[col], errors="coerce") #força para numérico

#categóricos
categorical_columns = [
    "Primary Typing", "Secondary Typing", "Generation", "Form", "Evolution Stage"
]

#Elimina pokemons com dados relevantes vazios
data.dropna(subset=numerical_columns + categorical_columns, inplace=True)
data.head()

# Define o pré-processamento
preprocessor = ColumnTransformer(transformers=[
    ("num", StandardScaler(), numerical_columns),
    ("cat", OneHotEncoder(handle_unknown="ignore"), categorical_columns) #transforma em binário
])
#Aplica o pré-processamento
pipeline = Pipeline(steps=[
    ("preprocessor", preprocessor),
    ("classifier", RandomForestClassifier(random_state=42))
])

#Parâmetros
param_distributions = {
    "classifier__n_estimators": [100, 200, 300, 500],
    "classifier__max_depth": [None, 10, 20, 30],
    "classifier__min_samples_split": [2, 5, 10],
    "classifier__min_samples_leaf": [1, 2, 4],
    "classifier__max_features": ["sqrt", "log2"],
}

#Entradas e saídas
X = data[categorical_columns + numerical_columns]
y = data["Legendary Status"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

search = RandomizedSearchCV(
    estimator=pipeline,
    param_distributions=param_distributions,
    n_iter=20,  # Número de combinações testadas
    cv=5,  # Validação cruzada
    scoring="f1_weighted",  # Ou "accuracy"
    random_state=42,
    #n_jobs=-1  # Usa todos os núcleos da CPU
)

# Treinamento com busca de hiperparâmetros
search.fit(X_train, y_train)

best_model = search.best_estimator_
print("Melhores hiperparâmetros encontrados:", search.best_params_)

# Avaliação no teste
predictions = best_model.predict(X_test)
acc = accuracy_score(y_test, predictions)
f1 = f1_score(y_test, predictions, average="weighted")
precision = precision_score(y_test, predictions, average="weighted")
recall = recall_score(y_test, predictions, average="weighted")

print(f"Acurácia: {acc:.4f}")
print(f"F1: {f1:.4f}")
print(f"Precisão: {precision:.4f}")
print(f"Recall: {recall:.4f}")